diff --git a/kill_vox_celebs.py b/kill_vox_celebs.py
index e69de29..a5ce4f6 100644
--- a/kill_vox_celebs.py
+++ b/kill_vox_celebs.py
@@ -0,0 +1,44 @@
+from src.utils.io_utils import ROOT_PATH, read_json, write_json
+from sklearn.model_selection import train_test_split
+from csv import DictReader
+from random import shuffle
+from pathlib import Path
+import os 
+
+def get_mp4_paths(directory='.'):
+    start_path = Path(directory)
+    mp4_list = [
+        str(file.absolute())
+        for file in start_path.rglob('*.mp4')
+    ]
+    vox_split = [{'path': '/'.join(full_path.split('/')[6:]), 'method': 'real'} for full_path in mp4_list if 'mouth_roi' not in full_path]
+    return vox_split
+
+def get_st_paths(directory='.'):
+    start_path = Path(directory)
+    mp4_list = [
+        str(file.absolute())
+        for file in start_path.rglob('*.safetensors')
+    ]
+    vox_split = [{'path': '/'.join(full_path.split('/')[6:]), 'method': 'real'} for full_path in mp4_list if 'mouth_roi' not in full_path]
+    return vox_split
+
+def create_vox_index():
+    dir_path = str(ROOT_PATH / 'data/VoxCelebTest/videos')
+    videos = get_mp4_paths(dir_path)
+    shuffle(videos)
+    index = videos[:3570]  # maybe crop to 3230 to get size of train 20'000
+    for path in videos[3570:]:
+        os.remove(str(ROOT_PATH / path['path']))
+    write_json(index, dir_path + '/index.json')
+
+def add_to_train():
+    dir_path = str(ROOT_PATH / 'data/VoxCelebTest/videos')
+    train_path = str(ROOT_PATH / 'data/fakeavcelebs/train/split.json')
+    train_split = read_json(train_path)
+    index = read_json(dir_path + '/index.json')
+    train_split += index
+    os.remove(train_path)
+    write_json(train_split, train_path)
+
+add_to_train()
\ No newline at end of file
diff --git a/src/configs/baseline.yaml b/src/configs/baseline.yaml
index 3e65111..12d1cbe 100644
--- a/src/configs/baseline.yaml
+++ b/src/configs/baseline.yaml
@@ -3,12 +3,12 @@ defaults:
   - writer: wandb
   - metrics: pages
   - datasets: fakeavcelebs
-  - dataloader: onebatchtest
+  - dataloader: dataloader
   - transforms: example
   - _self_
 optimizer:
   _target_: torch.optim.Adam
-  lr: 3e-4
+  lr: 1e-5
 lr_scheduler:
   _target_: torch.optim.lr_scheduler.StepLR
   gamma: 0.9
@@ -18,13 +18,13 @@ loss_function:
 trainer:
   log_step: 50
   n_epochs: 30
-  epoch_len: 4
+  epoch_len: 2545
   device_tensors: ["vivit_frames", "av_video", "av_audio", "aasist_audio", "labels"] # which tensors should be on device (ex. GPU)
   resume_from: null # null or path to the checkpoint dir with *.pth and config.yaml
   device: auto # device name or "auto"
-  override: False # if True, will override the previous run with the same name
+  override: True # if True, will override the previous run with the same name
   monitor: "max test_Accuracy" # "off" or "max/min metric_name", i.e. our goal is to maximize/minimize metric
-  save_period: 5 # checkpoint each save_period epochs in addition to the best epoch
+  save_period: 1 # checkpoint each save_period epochs in addition to the best epoch
   early_stop: ${trainer.n_epochs} # epochs for early stopping
-  save_dir: "saved"
+  save_dir: "first_run_mlp"
   seed: 1
diff --git a/src/configs/dataloader/onebatchtest.yaml b/src/configs/dataloader/onebatchtest.yaml
index 1d66049..0c212ff 100644
--- a/src/configs/dataloader/onebatchtest.yaml
+++ b/src/configs/dataloader/onebatchtest.yaml
@@ -1,4 +1,4 @@
 _target_: torch.utils.data.DataLoader
 batch_size: 4
-num_workers: 2
+num_workers: 4
 pin_memory: True
diff --git a/src/configs/datasets/fakeavcelebs.yaml b/src/configs/datasets/fakeavcelebs.yaml
index 69f82fc..159e6cf 100644
--- a/src/configs/datasets/fakeavcelebs.yaml
+++ b/src/configs/datasets/fakeavcelebs.yaml
@@ -2,35 +2,7 @@ train:
   _target_: src.datasets.FakeAVCelebsDataset
   name: "train"
   instance_transforms: ${transforms.instance_transforms.inference}
-test_faceswap:
-  _target_: src.datasets.FakeAVCelebsDataset
-  name: "test-faceswap"
-  instance_transforms: ${transforms.instance_transforms.inference}
-test_faceswap_wav2lip:
-  _target_: src.datasets.FakeAVCelebsDataset
-  name: "test-faceswap-wav2lip"
-  instance_transforms: ${transforms.instance_transforms.inference}
-test_fsgan:
-  _target_: src.datasets.FakeAVCelebsDataset
-  name: "test-fsgan"
-  instance_transforms: ${transforms.instance_transforms.inference}
-test_fsgan_Wav2lip:
-  _target_: src.datasets.FakeAVCelebsDataset
-  name: "test-fsgan-wav2lip"
-  instance_transforms: ${transforms.instance_transforms.inference}
-test_rtvc:
-  _target_: src.datasets.FakeAVCelebsDataset
-  name: "test-rtvc"
-  instance_transforms: ${transforms.instance_transforms.inference}
-test_set_1:
+test:
   _target_: src.datasets.FakeAVCelebsDataset
   name: "test-set-1"
-  instance_transforms: ${transforms.instance_transforms.inference}
-test_set_2:
-  _target_: src.datasets.FakeAVCelebsDataset
-  name: "test-set-2"
-  instance_transforms: ${transforms.instance_transforms.inference}
-test_wav2lip:
-  _target_: src.datasets.FakeAVCelebsDataset
-  name: "test-wav2lip"
   instance_transforms: ${transforms.instance_transforms.inference}
\ No newline at end of file
diff --git a/src/configs/onebatchtest.yaml b/src/configs/onebatchtest.yaml
index 76853b7..391935b 100644
--- a/src/configs/onebatchtest.yaml
+++ b/src/configs/onebatchtest.yaml
@@ -1,6 +1,6 @@
 defaults:
   - model: baseline
-  - writer: wandb
+  - writer: wandb_onebatch
   - metrics: pages
   - datasets: onebatchtest
   - dataloader: onebatchtest
@@ -22,7 +22,7 @@ trainer:
   device_tensors: ["vivit_frames", "av_video", "av_audio", "aasist_audio", "labels"] # which tensors should be on device (ex. GPU)
   resume_from: null # null or path to the checkpoint dir with *.pth and config.yaml
   device: auto # device name or "auto"
-  override: False # if True, will override the previous run with the same name
+  override: True # if True, will override the previous run with the same name
   monitor: "max test_Accuracy" # "off" or "max/min metric_name", i.e. our goal is to maximize/minimize metric
   save_period: 5 # checkpoint each save_period epochs in addition to the best epoch
   early_stop: ${trainer.n_epochs} # epochs for early stopping
diff --git a/src/configs/writer/wandb.yaml b/src/configs/writer/wandb.yaml
index b83b202..ec9b8ea 100644
--- a/src/configs/writer/wandb.yaml
+++ b/src/configs/writer/wandb.yaml
@@ -1,7 +1,7 @@
 _target_: src.logger.WandBWriter
 project_name: "pytorch_template"
 entity: null
-run_name: "testing"
+run_name: "baseline"
 mode: "online"
 loss_names: ["loss"] # which losses to log (useful for multi-loss tasks, s.a. GANs)
 log_checkpoints: False # set to True if you want to log save_dir with checkpoints to W&B
diff --git a/src/datasets/collate.py b/src/datasets/collate.py
index a8b3922..a4068ab 100644
--- a/src/datasets/collate.py
+++ b/src/datasets/collate.py
@@ -18,7 +18,7 @@ def av_video_pad(x, max_len = 75):
         return x[:max_len].clone()
     # if too short
     num_repeats = int(max_len / x_len) + 1
-    padded_x = x.repeat(num_repeats, 1, 1)[:max_len]
+    padded_x = x.repeat(num_repeats, 1, 1, 1)[:max_len]
     return padded_x.clone()
 
 def av_audio_pad(x, max_len = 75):
diff --git a/src/datasets/fakeavcelebs.py b/src/datasets/fakeavcelebs.py
index 0834938..c633755 100644
--- a/src/datasets/fakeavcelebs.py
+++ b/src/datasets/fakeavcelebs.py
@@ -60,6 +60,7 @@ class FakeAVCelebsDataset(BaseDataset):
         processor = Processor()
         current_index = 0
         failed = 0
+        regen = 0
         for i, row in tqdm(enumerate(elements), total=len(elements)):
             # create dataset
             st_path = processor.run(row)
@@ -69,6 +70,11 @@ class FakeAVCelebsDataset(BaseDataset):
                 print(f"Failed: {row['path']}")
                 continue
             element = safetensors.torch.load_file(st_path)
+            if element['vivit_frames'].shape[1] != 32:
+                regen += 1
+                print(f"Regenerating: { regen }")
+                processor.run(row, must=1)
+                element = safetensors.torch.load_file(st_path)
             element_path = data_path / f"{current_index:06}.safetensors"
             current_index += 1
             safetensors.torch.save_file(element, element_path)
diff --git a/src/datasets/preprocess.py b/src/datasets/preprocess.py
index 2dedb45..84ceaef 100644
--- a/src/datasets/preprocess.py
+++ b/src/datasets/preprocess.py
@@ -27,7 +27,7 @@ class ViViT_Processor():
     def __init__(self):
         self.image_processor = VivitImageProcessor.from_pretrained("google/vivit-b-16x2-kinetics400")
 
-    def vivit_preprocess(self, file_path):
+    def vivit_preprocess(self, file_path, must=0):
         """
         Args:
             file_path (str): filename of the mp4 file.
@@ -41,8 +41,11 @@ class ViViT_Processor():
         indices = self.sample_frame_indices(clip_len=32, frame_sample_rate=2, seg_len=container.streams.video[0].frames)
         video = self.read_video_pyav(container=container, indices=indices)
 
-        frames = self.image_processor(list(video), return_tensors="pt")
-        return frames.pixel_values.clone()
+        frames = self.image_processor(list(video), return_tensors="pt").pixel_values.clone()
+        if frames.shape[1] < 32:
+            num = 32 // frames.shape[1] + 1
+            frames = frames.repeat(1, num, 1, 1, 1)[:, :32, :, :]
+        return frames
 
 
     def read_video_pyav(self, container, indices):
@@ -79,7 +82,7 @@ class ViViT_Processor():
         '''
         converted_len = int(clip_len * frame_sample_rate)
         end_idx = np.random.randint(min(converted_len, seg_len-1), seg_len)
-        start_idx = end_idx - converted_len
+        start_idx = max(0, end_idx - converted_len)
         indices = np.linspace(start_idx, end_idx, num=clip_len)
         indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)
         return indices
@@ -202,16 +205,16 @@ class Processor:
         self.avp = AV_Processor()
         self.vvtp = ViViT_Processor()
 
-    def run(self, row):
+    def run(self, row, must=0):
         row_path = row['path']
         label = 1 if row['method'] == 'real' else 0
         
         st_path = str(ROOT_PATH / row_path.replace('mp4', 'safetensors'))
-        # if (os.path.exists(st_path)):
-        #     return st_path
-        video_fn, audio_fn = self.avp.av_preprocess(row_path)
+        if not must and os.path.exists(st_path):
+            return st_path
+        video_fn, audio_fn = self.avp.av_preprocess(row_path, must)
         av_frames, av_audio = self.hubert_load_feature(video_fn, audio_fn)
-        vivit_frames = self.vvtp.vivit_preprocess(row_path)
+        vivit_frames = self.vvtp.vivit_preprocess(row_path, must)
         aasist_audio = aasist_load(audio_fn)
         element = {
             "av_audio": av_crop(torch.from_numpy(av_audio)),
diff --git a/src/metrics/accuracy.py b/src/metrics/accuracy.py
index fbb5dd3..a11a665 100644
--- a/src/metrics/accuracy.py
+++ b/src/metrics/accuracy.py
@@ -32,4 +32,4 @@ class AccuracyMetric(BaseMetric):
         dclasses = logits.argmax(dim=-1).to(device)
         dlabels = labels.to(device)
         result = (dclasses == dlabels).mean(dtype=torch.float32)
-        return result
+        return result.item()
diff --git a/src/metrics/base_metric.py b/src/metrics/base_metric.py
index f83ebcf..b6bcbe0 100644
--- a/src/metrics/base_metric.py
+++ b/src/metrics/base_metric.py
@@ -12,6 +12,7 @@ class BaseMetric:
             name (str | None): metric name to use in logger and writer.
         """
         self.name = name if name is not None else type(self).__name__
+        self.is_global = False
 
     @abstractmethod
     def __call__(self, **batch):
diff --git a/src/metrics/precision.py b/src/metrics/precision.py
index e3a2054..40a47d8 100644
--- a/src/metrics/precision.py
+++ b/src/metrics/precision.py
@@ -17,6 +17,7 @@ class PrecisionMetric(BaseMetric):
             device (str): device for the metric calculation (and tensors).
         """
         super().__init__(*args, **kwargs)
+        self.is_global = True
 
     def __call__(self, logits: torch.Tensor, labels: torch.Tensor, **kwargs):
         """
@@ -33,5 +34,5 @@ class PrecisionMetric(BaseMetric):
         dlabels = labels.to(device)
         true_positive = ((dclasses == dlabels) * dlabels).sum(dtype=torch.float32)
         false_positive = ((dclasses != dlabels) * (1 - dlabels)).sum(dtype=torch.float32)
-        return true_positive / (true_positive + false_positive)
+        return true_positive.item(), (true_positive + false_positive).item()
     
diff --git a/src/metrics/recall.py b/src/metrics/recall.py
index d9e40e7..f83841b 100644
--- a/src/metrics/recall.py
+++ b/src/metrics/recall.py
@@ -17,6 +17,7 @@ class RecallMetric(BaseMetric):
             device (str): device for the metric calculation (and tensors).
         """
         super().__init__(*args, **kwargs)
+        self.is_global = True
 
     def __call__(self, logits: torch.Tensor, labels: torch.Tensor, **kwargs):
         """
@@ -33,4 +34,4 @@ class RecallMetric(BaseMetric):
         dlabels = labels.to(device)
         true_positive = ((dclasses == dlabels) * dlabels).sum(dtype=torch.float32)
         false_negative = ((dclasses != dlabels) * dlabels).sum(dtype=torch.float32)
-        return true_positive / (true_positive + false_negative)
+        return true_positive.item(), (true_positive + false_negative).item()
diff --git a/src/metrics/tracker.py b/src/metrics/tracker.py
index 314be33..4b0f4a0 100644
--- a/src/metrics/tracker.py
+++ b/src/metrics/tracker.py
@@ -40,7 +40,37 @@ class MetricTracker:
         self._data.loc[key, "total"] += value * n
         self._data.loc[key, "counts"] += n
         self._data.loc[key, "average"] = self._data.total[key] / self._data.counts[key]
+    
+    def update(self, key, value, n=1):
+        """
+        Update metrics DataFrame with new value.
+
+        Args:
+            key (str): metric name.
+            value (float): metric value on the batch.
+            n (int): how many times to count this value.
+        """
+        # if self.writer is not None:
+        #     self.writer.add_scalar(key, value)
+        self._data.loc[key, "total"] += value * n
+        self._data.loc[key, "counts"] += n
+        self._data.loc[key, "average"] = self._data.total[key] / self._data.counts[key]
+
+    def update_global(self, key, num, denum):
+        """
+        Update metrics DataFrame with new value.
 
+        Args:
+            key (str): metric name.
+            value (float): metric value on the batch.
+            n (int): how many times to count this value.
+        """
+        # if self.writer is not None:
+        #     self.writer.add_scalar(key, value)
+        self._data.loc[key, "total"] += num
+        self._data.loc[key, "counts"] += denum
+        self._data.loc[key, "average"] = 0 if self._data.counts[key] == 0 else self._data.total[key] / self._data.counts[key]
+    
     def avg(self, key):
         """
         Return average value for a given metric.
diff --git a/src/model/av_hubert b/src/model/av_hubert
--- a/src/model/av_hubert
+++ b/src/model/av_hubert
@@ -1 +1 @@
-Subproject commit 258fb50e155134eec2c4b49c2ae8de267075fd18
+Subproject commit 258fb50e155134eec2c4b49c2ae8de267075fd18-dirty
diff --git a/src/model/baseline_model.py b/src/model/baseline_model.py
index ead6bba..7f7e5ee 100644
--- a/src/model/baseline_model.py
+++ b/src/model/baseline_model.py
@@ -22,6 +22,8 @@ class BaselineModel(nn.Module):
         """
         super().__init__()
 
+        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
+
         # init av-hubert model
         # first time need to run this command: fairseq.utils.import_user_module(Namespace(user_dir='/home/runtime57/hse/coursework_2/deepfake_detection/src/model/av_hubert/avhubert'))
         ckpt_path = '/home/runtime57/hse/coursework_2/deepfake_detection/src/model/av_hubert/ckpt/base_vox_433h.pt'
@@ -32,13 +34,17 @@ class BaselineModel(nn.Module):
             self.avhubert = models[0].encoder.w2v_model
         else:
             print(f"Checkpoint: pre-trained w/o fine-tuning")
-        self.avhubert.cuda()
+        self.avhubert = self.avhubert.to(self.device)
         self.avhubert.eval()
         
-        self.vivit = VivitModel.from_pretrained("google/vivit-b-16x2-kinetics400").eval()
-        
+        self.vivit = VivitModel.from_pretrained("google/vivit-b-16x2-kinetics400")
+        self.vivit = self.vivit.to(self.device)
+        self.vivit.eval()
+
         self.aasist = aasist_encoder()
-        
+        self.aasist = self.aasist.to(self.device)
+        self.aasist.eval()
+
         self.mlp = Sequential(
             nn.Linear(in_features=av_channels+vivit_channels+as_channels, out_features=hidden_channels),
             nn.ReLU(),
@@ -57,10 +63,12 @@ class BaselineModel(nn.Module):
             output (dict): output dict containing logits.
         """
         
+
+        as_feats, av_feats, vivit_feats = self._extract_feats(vivit_frames, av_video.float(), av_audio.float(), aasist_audio)
+
         as_feats = self.aasist(aasist_audio)
         as_pooled_feats = as_feats.mean(dim=1)
 
-        av_feats, vivit_feats = self._extract_feats(vivit_frames, av_video.float(), av_audio.float())
         av_pooled_feats = av_feats.mean(dim=1)
 
         feats = torch.cat([av_pooled_feats, vivit_feats, as_pooled_feats], dim=1)
@@ -82,9 +90,10 @@ class BaselineModel(nn.Module):
 
         return result_info
 
-    def _extract_feats(self, vivit_frames, av_video, av_audio):
+    def _extract_feats(self, vivit_frames, av_video, av_audio, aasist_audio):
          with torch.no_grad():
             # print(av_video.device, av_audio.device, av_mask.device)
+            as_feats = self.aasist(aasist_audio)
             av_feats, _ = self.avhubert.extract_finetune(source={'video': av_video, 'audio': av_audio}, padding_mask=None, output_layer=None)
             vivit_feats = self.vivit(pixel_values=vivit_frames).last_hidden_state[:, 0, :]
-            return av_feats, vivit_feats
+            return as_feats, av_feats, vivit_feats
diff --git a/src/trainer/base_trainer.py b/src/trainer/base_trainer.py
index 74292b6..e69b168 100644
--- a/src/trainer/base_trainer.py
+++ b/src/trainer/base_trainer.py
@@ -29,7 +29,7 @@ class BaseTrainer:
         writer,
         epoch_len=None,
         skip_oom=True,
-        batch_transforms=None,
+        batch_transforms=None
     ):
         """
         Args:
diff --git a/src/trainer/trainer.py b/src/trainer/trainer.py
index cbedf25..c974308 100644
--- a/src/trainer/trainer.py
+++ b/src/trainer/trainer.py
@@ -52,7 +52,11 @@ class Trainer(BaseTrainer):
             metrics.update(loss_name, batch[loss_name].item())
 
         for met in metric_funcs:
-            metrics.update(met.name, met(**batch).item())
+            if met.is_global:
+                num, denum = met(**batch)
+                metrics.update_global(met.name, num, denum)
+            else:
+                metrics.update(met.name, met(**batch))
         return batch
 
     def _log_batch(self, batch_idx, batch, mode="train"):
diff --git a/src/utils/split_utils.py b/src/utils/split_utils.py
index 4d6f32a..b80438c 100644
--- a/src/utils/split_utils.py
+++ b/src/utils/split_utils.py
@@ -15,6 +15,10 @@ def get_mp4_paths(directory='.'):
     vox_split = [{'path': '/'.join(full_path.split('/')[6:]), 'method': 'real'} for full_path in mp4_list if 'mouth_roi' not in full_path]
     return vox_split
 
+def add_to_train():
+    dir_path = str(ROOT_PATH / 'data/VoxCelebTest/videos')
+    index = read_json(dir_path + '/index.json')
+    return index
 
 def generate_split(random_state=79098):
     """
@@ -82,6 +86,7 @@ def generate_split(random_state=79098):
 
     train_path = ROOT_PATH / "data" / "fakeavcelebs" / "train"
     train_path.mkdir(exist_ok=True, parents=True)
+    train += add_to_train()
     # train = get_mp4_paths(str(ROOT_PATH / 'data/VoxCelebTest'))[:3570] + train
     write_json(train, str(train_path / "split.json"))
 
